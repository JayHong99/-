{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implement Word 2 Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Tuple\n",
    "from random import randint\n",
    "import re\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "import pandas as pd\n",
    "train = pd.read_csv('../../../../data/klue clf/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>인천→핀란드 항공기 결항…휴가철 여행객 분통</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>시진핑 트럼프에 중미 무역협상 조속 타결 희망</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                             title  topic_idx\n",
       "0      0          인천→핀란드 항공기 결항…휴가철 여행객 분통          4\n",
       "1      1    실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화          4\n",
       "2      2    이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것          4\n",
       "3      3  NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합          4\n",
       "4      4         시진핑 트럼프에 중미 무역협상 조속 타결 희망          4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Make Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence) : \n",
    "    sentence = re.sub('[.]', ' . ', sentence)\n",
    "    sentence = re.sub('[,]', ' , ', sentence)\n",
    "    sentence = re.sub('[!]', ' ! ', sentence)\n",
    "    sentence = re.sub('[?]', ' ? ', sentence)\n",
    "    tokens = sentence.split(' ')\n",
    "    \n",
    "    assert type(tokens) == list\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw\n",
      "의총에서 발언하는 이정미\n",
      "Tokenize\n",
      "['의총에서', '발언하는', '이정미']\n",
      "\n",
      "Raw\n",
      "아스널 지루 허벅지 부상… 프랑스축구협회 독일과 A매치 결장\n",
      "Tokenize\n",
      "['아스널', '지루', '허벅지', '부상…', '프랑스축구협회', '독일과', 'A매치', '결장']\n",
      "\n",
      "Raw\n",
      "치유·감동의 하모니 제주국제합창축제 막 올라\n",
      "Tokenize\n",
      "['치유·감동의', '하모니', '제주국제합창축제', '막', '올라']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Tokenize\n",
    "sample_sentences = train.sample(frac = 1).iloc[:3].title.values\n",
    "\n",
    "for sentence in sample_sentences : \n",
    "    print('Raw')\n",
    "    print(sentence)\n",
    "    print('Tokenize')\n",
    "    print(tokenize(sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences : List[List[str]], min_freq : int) -> Tuple[List[str], Dict[str,int], List[int]] :\n",
    "    \"\"\"\n",
    "    input\n",
    "    sentences\n",
    "    min_freq\n",
    "\n",
    "    return\n",
    "    idx2word\n",
    "    word2idx\n",
    "    word_freq\n",
    "    \"\"\"\n",
    "    PAD = '<PAD>'\n",
    "    PAD_IDX = 0\n",
    "    UNK = '<UNK>'\n",
    "    UNK_IDX = 1\n",
    "\n",
    "    word2idx = {PAD : PAD_IDX, UNK : UNK_IDX}\n",
    "    idx2word = {PAD_IDX : PAD, UNK_IDX : UNK}\n",
    "\n",
    "    flatten = lambda x : [item for sublist in x for item in sublist]\n",
    "\n",
    "    word_freq = dict(Counter(flatten(sentences)))\n",
    "    \n",
    "    for word, freq in word_freq.items() : \n",
    "        if freq < min_freq : \n",
    "            continue;\n",
    "        else : \n",
    "            word2idx[word] = len(word2idx)\n",
    "            idx2word[len(idx2word)] = word\n",
    "\n",
    "    word_freq = {PAD : PAD_IDX, UNK : UNK_IDX}\n",
    "    for word_list in sentences : \n",
    "        for word in word_list : \n",
    "            if word not in word2idx : \n",
    "                word_freq[UNK] += 1\n",
    "            else : \n",
    "                try : \n",
    "                    word_freq[word] += 1\n",
    "                except KeyError: \n",
    "                    word_freq[word] = 1\n",
    "\n",
    "    word_freq = list(word_freq.values())\n",
    "    idx2word = list(idx2word.values())\n",
    "\n",
    "    assert idx2word[PAD_IDX] == PAD and word2idx[PAD] == PAD_IDX, \\\n",
    "        \"PAD token should be placed properly\"\n",
    "    assert idx2word[UNK_IDX] == UNK and word2idx[UNK] == UNK_IDX, \\\n",
    "        \"UNK token should be placed properly\"\n",
    "    assert len(idx2word) == len(word2idx) and len(idx2word) == len(word_freq), \\\n",
    "        \"Size of idx2word, word2idx and word_freq should be same\"\n",
    "\n",
    "    return idx2word, word2idx, word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = list(train['title'].apply(tokenize).values)\n",
    "idx2word, word2idx, word_freq = build_vocab(tokenized_sentences, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Build Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(sentence, window_size, center_word_loc) : \n",
    "    outside_words = []\n",
    "\n",
    "    for w in range(-window_size, window_size + 1) : \n",
    "        context_word_loc = center_word_loc + w\n",
    "            \n",
    "        if context_word_loc < 0 or context_word_loc >= len(sentence) or center_word_loc == context_word_loc : \n",
    "            continue;\n",
    "        outside_words.append(sentence[context_word_loc])\n",
    "\n",
    "        assert type(sentence[center_word_loc]) == str and type(outside_words) == list\n",
    "    return sentence[center_word_loc], outside_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('진전', ['푸틴', '한반도', '상황', '위한', '방안', '김정은'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram(tokenized_sentences[8], window_size=3, center_word_loc=3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
